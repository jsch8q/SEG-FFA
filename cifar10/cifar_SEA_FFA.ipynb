{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgQbxzZN44xS",
        "outputId": "abc1b944-ebd3-43da-ed6b-4d3cbe7411e3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor  \n",
        "import os\n",
        "\n",
        "torch.manual_seed(1)\n",
        "torch.set_float32_matmul_precision('high')\n",
        "default_type = torch.get_default_dtype()\n",
        "\n",
        "cuda = torch.device(\"cuda\")\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SZkNWLd44xU"
      },
      "source": [
        "***\n",
        "### Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaqIUBZ744xV",
        "outputId": "b9046e17-c0f5-4fb7-c467-4d63b5c39422"
      },
      "outputs": [],
      "source": [
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "hd3F9pcZ44xV",
        "outputId": "42942281-20a5-4ec2-d201-277c5c428df9"
      },
      "outputs": [],
      "source": [
        "X_data = (training_data.data / 256).reshape(-1, 32*32*3)\n",
        "mns = np.zeros((10, 32*32*3))\n",
        "dists = np.zeros((10, 10))\n",
        "for i in range(10):\n",
        "    mns[i] = np.mean(X_data[np.array(training_data.targets) == i, :], axis=0)\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        dists[i][j] = np.linalg.norm(mns[i] - mns[j])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(dists)\n",
        "ax.set_xticks(np.arange(10))\n",
        "ax.set_yticks(np.arange(10))\n",
        "\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        text = ax.text(j, i, \"%.2f\" %(dists[i, j]),\n",
        "                       ha=\"center\", va=\"center\", color=\"w\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cz2sSwJD44xW"
      },
      "outputs": [],
      "source": [
        "in_channel = 3 # 1 if grayscale, 3 if full color\n",
        "classes = [3, 4, 5]  # categories of consideration\n",
        "num_class = len(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15h0qQEm44xW"
      },
      "outputs": [],
      "source": [
        "# Extract data and normalize colors to [0, 1]\n",
        "X_train_full = np.moveaxis(training_data.data / 256, -1, 1)\n",
        "y_train_full = np.array(training_data.targets)\n",
        "in_class_train = np.isin(y_train_full, classes)\n",
        "X_train_full = torch.tensor(X_train_full[in_class_train], dtype=default_type, device=cuda)\n",
        "y_train_full = y_train_full[in_class_train]\n",
        "y_train_full = torch.tensor([classes.index(y) for y in y_train_full], device=cuda)\n",
        "\n",
        "X_test_full = np.moveaxis(test_data.data / 256, -1, 1)\n",
        "y_test_full = np.array(test_data.targets)\n",
        "in_class_test = np.isin(y_test_full, classes)\n",
        "X_test_full = torch.tensor(X_test_full[in_class_test], dtype=default_type, device=cuda)\n",
        "y_test_full = y_test_full[in_class_test]\n",
        "y_test_full = torch.tensor([classes.index(y) for y in y_test_full], device=cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtyMfQwp44xW",
        "outputId": "dd31015e-bef1-48d1-a6c6-d9c2c400e0e8"
      },
      "outputs": [],
      "source": [
        "X_train_full.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "62InGzTJ44xW",
        "outputId": "1d64cb26-f7e8-46e2-d8eb-df72e717dcd5"
      },
      "outputs": [],
      "source": [
        "plt.imshow(np.moveaxis(X_train_full[1].cpu().numpy(), 0, -1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_vjH4Vx44xX"
      },
      "outputs": [],
      "source": [
        "# Generate minibatches for shuffling\n",
        "n_sample = len(X_train_full)\n",
        "batch_size = 128\n",
        "X_train_split = torch.split(X_train_full, batch_size)\n",
        "y_train_split = torch.split(y_train_full, batch_size)\n",
        "Y_batch_split = tuple(map(lambda y_batch: torch.stack([torch.eye(num_class)[y] for y in y_batch]).to(cuda), y_train_split))\n",
        "n_batch = len(y_train_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jknW3QT44xX"
      },
      "source": [
        "***\n",
        "### Constructing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN6kcVim44xX"
      },
      "outputs": [],
      "source": [
        "class cnn(nn.Module):\n",
        "    def __init__(self, in_channel=1, num_class=3):\n",
        "        super().__init__()\n",
        "        self.C1_1 = nn.Conv2d(in_channels=in_channel, out_channels=16, kernel_size=3, padding=1)  # 16*32*32\n",
        "        self.A1_1 = nn.ReLU()\n",
        "        self.C1_2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)  # 16*32*32\n",
        "        self.A1_2 = nn.ReLU()\n",
        "        self.P1 = nn.MaxPool2d(kernel_size=2)  # 16*16*16\n",
        "\n",
        "        self.C2_1 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)  # 32*16*16\n",
        "        self.A2_1 = nn.ReLU()\n",
        "        self.C2_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)  # 32*16*16\n",
        "        self.A2_2 = nn.ReLU()\n",
        "        self.C2_3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)  # 32*16*16\n",
        "        self.A2_3 = nn.ReLU()\n",
        "        self.P2 = nn.MaxPool2d(kernel_size=2)  # 32*8*8\n",
        "\n",
        "        self.Flt = nn.Flatten()\n",
        "        self.L3 = nn.Linear(in_features=2048, out_features=128)\n",
        "        self.A3 = nn.ReLU()\n",
        "\n",
        "        self.L4 = nn.Linear(in_features=128, out_features=num_class)\n",
        "        self.A4 = nn.Tanh()\n",
        "\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x) :\n",
        "        out = self.C1_1(x)\n",
        "        out = self.A1_1(out)\n",
        "        out = self.C1_2(out)\n",
        "        out = self.A1_2(out)\n",
        "        out = self.P1(out)\n",
        "\n",
        "        out = self.C2_1(out)\n",
        "        out = self.A2_1(out)\n",
        "        out = self.C2_2(out)\n",
        "        out = self.A2_2(out)\n",
        "        out = self.C2_3(out)\n",
        "        out = self.A2_3(out)\n",
        "        out = self.P2(out)\n",
        "\n",
        "        out = self.Flt(out)\n",
        "        out = self.L3(out)\n",
        "        out = self.A3(out)\n",
        "\n",
        "        out = self.L4(out)\n",
        "        # out = self.A4(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        pred = self.forward(x)\n",
        "        return self.loss_fn(pred, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcvSobuC44xX"
      },
      "source": [
        "***\n",
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnfpy8Nm44xX"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(X_test, y_test, n_cat, model):\n",
        "    test_pred = torch.argmax(F.forward(X_test), dim=1)\n",
        "    correct = (test_pred == y_test).type(torch.float)\n",
        "    acc = torch.mean(correct)\n",
        "    cat_acc = [torch.mean(correct[y_test == c]).item() for c in range(n_cat)]\n",
        "    return acc, cat_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luo2F2UG44xX"
      },
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "n_runs = 5\n",
        "print_skip = 5 if (epochs >= 100) else 5\n",
        "worst_acc_track = torch.zeros((n_runs, epochs//print_skip + 1))\n",
        "loss_track = torch.zeros((n_runs, epochs//print_skip + 1))\n",
        "\n",
        "verbose = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhcO0S_j44xX",
        "outputId": "234ab60c-cbcd-4c45-a137-ea5e747811d7"
      },
      "outputs": [],
      "source": [
        "for run in range(n_runs):\n",
        "    print(\"--- Experiment #%2d ---\\n\" %(run+1))\n",
        "\n",
        "    # Initialize NN and parameters\n",
        "    F = cnn(in_channel, num_class).to(cuda)\n",
        "    default_type = torch.get_default_dtype()\n",
        "    theta = torch.rand(1, dtype=default_type, requires_grad=True, device=cuda)\n",
        "    phi = torch.rand(1, dtype=default_type, requires_grad=True, device=cuda)\n",
        " \n",
        "    min_grad_optim = torch.optim.Adam(F.parameters(), lr=0.002)\n",
        "    max_grad_optim = torch.optim.Adam([theta, phi], lr=0.002, maximize=True)\n",
        "\n",
        "    eta_k = lambda k: 1.0 / ((1 + 0.05*k)**0.34) \n",
        "    min_grad_sched = torch.optim.lr_scheduler.LambdaLR(min_grad_optim, eta_k)\n",
        "    max_grad_sched = torch.optim.lr_scheduler.LambdaLR(max_grad_optim, eta_k)\n",
        "\n",
        "    # Get info on the untrained NN\n",
        "    pred = -torch.nn.functional.log_softmax(F.forward(X_train_split[0]), dim=1)\n",
        "    cnt = torch.sum(Y_batch_split[0], dim=0)\n",
        "    per_class_loss = torch.sum(pred * Y_batch_split[0], dim=0) / cnt\n",
        "    z = torch.concat((torch.cos(theta),\n",
        "                      torch.sin(theta)*torch.cos(phi),\n",
        "                      torch.sin(theta)*torch.sin(phi)))\n",
        "    x = z * z\n",
        "    loss = torch.inner(per_class_loss, x)\n",
        "\n",
        "    acc, class_acc = get_accuracy(X_test_full, y_test_full, num_class, F)\n",
        "    worst_acc = min(class_acc)\n",
        "    print(\"epoch %d : train loss %.4f with [%.4f, %.4f, %.4f],\"\n",
        "          %(0, loss.item(), per_class_loss[0], per_class_loss[1], per_class_loss[2]))\n",
        "    print(\"\\t\\t test accuracy %.4f/%.4f with [%.4f, %.4f, %.4f]\"\n",
        "          %(worst_acc, acc, class_acc[0], class_acc[1], class_acc[2]))\n",
        "    worst_acc_track[run, 0] = worst_acc\n",
        "    loss_track[run, 0] = loss.item()\n",
        "\n",
        "    for i in range(epochs):\n",
        "        tau1 = torch.randperm(n_batch)\n",
        "        tau = torch.concat((tau1, torch.flip(tau1, dims=(0,))))\n",
        "\n",
        "        # Save anchors\n",
        "        min_params_anch = []\n",
        "        max_params_anch = []\n",
        "        for W in F.parameters():\n",
        "            min_params_anch.append(W.clone().detach())\n",
        "        max_params_anch.append(theta.clone().detach())\n",
        "        max_params_anch.append(phi.clone().detach())\n",
        "\n",
        "        # Flip-flop concatenated into tau\n",
        "        for bb in range(2*n_batch):\n",
        "            batch_idx = tau[bb]\n",
        "            X_train_batch = X_train_split[batch_idx]\n",
        "            y_train_batch = y_train_split[batch_idx]\n",
        "            Y_batch = Y_batch_split[batch_idx]\n",
        "            cnt = torch.sum(Y_batch, dim=0)\n",
        "\n",
        "            ## Save parameters for gradient step\n",
        "            min_params = []\n",
        "            max_params = []\n",
        "            for W in F.parameters():\n",
        "                min_params.append(W.clone().detach())\n",
        "            max_params.append(theta.clone().detach())\n",
        "            max_params.append(phi.clone().detach())\n",
        "\n",
        "            ## Extrapolation Step\n",
        "            for pg in min_grad_optim.param_groups:\n",
        "                pg['lr'] *= .5\n",
        "            for pg in max_grad_optim.param_groups:\n",
        "                pg['lr'] *= .5\n",
        "            min_grad_optim.zero_grad()\n",
        "            max_grad_optim.zero_grad()\n",
        "            pred = -torch.nn.functional.log_softmax(F.forward(X_train_batch), dim=1)\n",
        "            per_class_loss = torch.sum(pred * Y_batch, dim=0) / cnt\n",
        "\n",
        "            z = torch.concat((torch.cos(theta),\n",
        "                              torch.sin(theta)*torch.cos(phi),\n",
        "                              torch.sin(theta)*torch.sin(phi)))\n",
        "            x = z * z\n",
        "\n",
        "            loss = torch.inner(per_class_loss, x)\n",
        "            loss.backward()\n",
        "            min_grad_optim.step()\n",
        "            max_grad_optim.step()\n",
        "\n",
        "            ## Gradient Step\n",
        "            for pg in min_grad_optim.param_groups:\n",
        "                pg['lr'] *= 2.\n",
        "            for pg in max_grad_optim.param_groups:\n",
        "                pg['lr'] *= 2.\n",
        "            min_grad_optim.zero_grad()\n",
        "            max_grad_optim.zero_grad()\n",
        "            pred = -torch.nn.functional.log_softmax(F.forward(X_train_batch), dim=1)\n",
        "            per_class_loss = torch.sum(pred * Y_batch, dim=0) / cnt\n",
        "\n",
        "            z = torch.concat((torch.cos(theta),\n",
        "                              torch.sin(theta)*torch.cos(phi),\n",
        "                              torch.sin(theta)*torch.sin(phi)))\n",
        "            x = z * z\n",
        "\n",
        "            loss = torch.inner(per_class_loss, x)\n",
        "            loss.backward()\n",
        "\n",
        "            ### Recover parameters before taking a step\n",
        "            for W0, W in zip(min_params, F.parameters()):\n",
        "                W.data = W0.data\n",
        "            theta.data = max_params[0].data\n",
        "            phi.data = max_params[1].data\n",
        "\n",
        "            min_grad_optim.step()\n",
        "            max_grad_optim.step()\n",
        "\n",
        "        # Averaging with anchors\n",
        "        for W0, W in zip(min_params_anch, F.parameters()):\n",
        "            W.data = 0.5 * (W0.data + W.data)\n",
        "        for W0, W in zip(max_params_anch, [theta, phi]):\n",
        "            W.data = 0.5 * (W0.data + W.data)\n",
        "\n",
        "        # Stepsize decay \n",
        "        min_grad_sched.step()\n",
        "        max_grad_sched.step()\n",
        "\n",
        "        # Print out intermediate results\n",
        "        if verbose and (i+1) % print_skip == 0 :\n",
        "            acc, class_acc = get_accuracy(X_test_full, y_test_full, num_class, F)\n",
        "            worst_acc = min(class_acc)\n",
        "            print(\"epoch %d : lr = %.4f, train loss %.4f with [%.4f, %.4f, %.4f],\"\n",
        "                  %(i+1, min_grad_sched.get_last_lr()[0], loss.item(),\n",
        "                    per_class_loss[0], per_class_loss[1], per_class_loss[2]))\n",
        "            print(\"\\t\\t test accuracy %.4f/%.4f with [%.4f, %.4f, %.4f]\"\n",
        "                  %(worst_acc, acc, class_acc[0], class_acc[1], class_acc[2]))\n",
        "            worst_acc_track[run, (i+1)//print_skip] = worst_acc\n",
        "            loss_track[run, (i+1)//print_skip] = loss.item()\n",
        "\n",
        "    # Print newline between runs\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-a_bRoE44xY"
      },
      "outputs": [],
      "source": [
        "result_dir = \"./results/\"\n",
        "if not(os.path.exists(result_dir)):\n",
        "    os.mkdir(result_dir)\n",
        "\n",
        "np.save(\"./results/cifar_SEA_FFA_results\", worst_acc_track, allow_pickle=False)\n",
        "np.save(\"./results/cifar_SEA_FFA_losses\", loss_track, allow_pickle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "z0WmOtiz44xY",
        "outputId": "63e4d147-5432-44dc-875e-234e44024f65"
      },
      "outputs": [],
      "source": [
        "std, avg = torch.std_mean(worst_acc_track, dim=0, keepdim=False)\n",
        "avg = avg.cpu().detach().numpy()\n",
        "std = std.cpu().detach().numpy()\n",
        "fig, ax = plt.subplots()\n",
        "iters = np.arange(0, epochs+1, print_skip)\n",
        "ax.plot(iters, avg)\n",
        "ax.fill_between(iters, avg-2*std, avg+2*std, alpha=0.5)\n",
        "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "ax.set_ylim(0.5, 0.9)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eBXAAic44xY",
        "outputId": "bdb050b9-d885-476b-b6f4-feef246723db"
      },
      "outputs": [],
      "source": [
        "ss = np.vstack((iters, worst_acc_track))\n",
        "np.set_printoptions(linewidth=1000)\n",
        "ss[:,10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5s2EE7p44xY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
